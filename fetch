#!/usr/bin/env ruby

require "nokogiri"
require "net/http"
require "optparse"
require "fileutils"
require "uri"
require "json"

class WebsiteFetcher
  class BadResponseError < StandardError
  end

  def initialize(urls = [], flags = {})
    @urls = urls
    @flags = flags
  end

  def perform
    for url in @urls
      begin
        @parsed_url = parse_url(url)
        html_body = fetch_html
        metadata_hash = fetch_metadata(html_body)
        save_to_local(html_body: html_body, metadata: metadata_hash)
      rescue StandardError => e
        $stderr.puts "Process Filed for: #{url}\n"
        $stderr.puts "#{e.message}\n\n"
        next
      end
    end
  end

  private

  def save_to_local(html_body:, metadata:)
    # .strftime("%a %b %d %Y %H:%M %Z")
    FileUtils.mkdir_p @parsed_url.host
    file_data = []
    if File.exists?("#{@parsed_url.host}/metadata.json")
      file_data = JSON.parse(File.read("#{@parsed_url.host}/metadata.json"))
    end
    previous_fetch_time = nil
    previous_fetch_time = file_data[-1]["fetch_time"] if !file_data.empty?

    file_data.push metadata
    File.open("#{@parsed_url.host}/metadata.json", "w") do |f|
      f.write(file_data.to_json)
    end
    File.open("#{@parsed_url.host}/#{@parsed_url.host}.html", "w") do |f|
      f.write(html_body)
    end
    if @flags[:show_metadata]
      $stdout.puts metadata_string_builder(
                     metadata_hash: metadata,
                     previous_fetch_time: previous_fetch_time,
                   )
    end
  end
  #
  # Fetch the html source of the target website
  # @return [String] html body
  #
  def fetch_html
    response = Net::HTTP.get_response(@parsed_url)
    if response.code.to_i > 299
      raise BadResponseError,
            "Response Status:[#{response.code}] for #{@parsed_url}"
    end
    response.body
  end
  #
  # Fetch the metadata of the target website
  # @return nil
  #
  def fetch_metadata(source)
    parsed_source = Nokogiri.HTML(source)
    link_count = parsed_source.xpath("//a[@href]").count
    image_count = parsed_source.xpath("//img[@src]").count
    metadata_hash = {
      site: @parsed_url.host,
      num_links: link_count,
      images: image_count,
      fetch_time: Time.now.utc.strftime("%a %b %d %Y %H:%M:%S %Z"),
    }
  end

  #
  # Build the print string of the metadata
  # @return nil
  #
  def metadata_string_builder(metadata_hash:, previous_fetch_time:)
    sb = ""
    metadata_hash.each { |key, value| sb += "#{key}: #{value}\n" }
    sb += "previous_fetch_time: #{previous_fetch_time || "--"}\n"
    sb += "\n"
  end

  #
  # Validate and parse the url using URI.parse
  # @return parsed URI
  #
  def parse_url(url)
    parsed_url = URI.parse(url)
    return parsed_url unless parsed_url.host.nil?
    raise URI::InvalidURIError, "Invalid URL: #{url}"
  end
end

# Driver Method
if __FILE__ == $0
  flags = {}

  opt = OptionParser.new
  opt.on("-h", "--help", "Help") do |v|
    $stdout.puts "Supported flags:
    -m / --metadata: Display the metadata of the target website
    -h / --help:     Show all supported flags"
    exit 0
  end
  opt.on("-m", "--metadata", "Display Metadata") do |v|
    flags[:show_metadata] = true
  end
  opt.parse!(ARGV)

  if ARGV.count < 1
    $stderr.puts "Please input the url of target website. \nTry \"./fetch https://www.google.com/\""
    exit 1
  end

  WebsiteFetcher.new(ARGV, flags).perform
end
